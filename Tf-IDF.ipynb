{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.sql.functions import col,split\n",
    "from pyspark.ml.feature import StopWordsRemover,Tokenizer,CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF,IDF,IDFModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import SVMModel,SVMWithSGD\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD,SVMWithSGD\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_MEMORY='8g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"P\") \\\n",
    "       .config(\"spark.some.config.option\",\"some-value\") \\\n",
    "       .config(\"spark.executor.memory\",MAX_MEMORY)\\\n",
    "       .config(\"spark.driver.memory\",MAX_MEMORY)\\\n",
    "       .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test=pd.read_csv('test.csv')\n",
    "m=pd.read_csv('mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata=spark.createDataFrame(train)\n",
    "tedata=spark.createDataFrame(test)\n",
    "mdata=spark.createDataFrame(m,['id','genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata=tdata.withColumn(\"plot\",f.regexp_replace(f.col(\"plot\"),r'[^A-Za-z0-9 ]',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt = RegexTokenizer(inputCol = \"plot\", outputCol = \"o1\", pattern = \"\\\\W\" )\n",
    "tdata=rt.transform(tdata)\n",
    "remover = StopWordsRemover(inputCol = \"o1\" ,outputCol=\"so2\")\n",
    "tdata=remover.transform(tdata)\n",
    "numfeatures=32\n",
    "htf=HashingTF(inputCol=\"so2\",outputCol=\"rfeatures\",numFeatures=382)\n",
    "tdata=htf.transform(tdata)\n",
    "idf=IDF(inputCol=\"rfeatures\",outputCol=\"features\")\n",
    "idfmodel=idf.fit(tdata)\n",
    "tdata1=idfmodel.transform(tdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tdata1.select(\"features\").show()\n",
    "#for i in tdata1.collect():\n",
    " #   print(type(i))\n",
    "m1=mdata.select(\"genre\",\"id\").rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=tdata.select('genre')\n",
    "m1=mdata.select(\"genre\",\"id\").rdd.collectAsMap()\n",
    "z=y.withColumn('genre',regexp_replace(\"genre\",r'[^A-za-z ,/-]',\"\"))\n",
    "\n",
    "x=[]\n",
    "for r in z.rdd.collect(): \n",
    "    g=r['genre']\n",
    "    g=re.sub(r'[^A-Za-z ,/-]',\"\",g)\n",
    "    g=list(g.split(\",\"))\n",
    "    yo=[]\n",
    "    for c in g:\n",
    "        c=c.strip()\n",
    "        yo.append(m1[c])\n",
    "    iz=[]\n",
    "    for izx in range(0,len(m1)): \n",
    "        iz.append(0)\n",
    "    #print(iz)\n",
    "    for iz1 in yo:\n",
    "        #print(iz1)\n",
    "        iz[iz1]=1\n",
    "        \n",
    "    x.append(iz)\n",
    "yu=spark.createDataFrame(x,ArrayType(IntegerType()))\n",
    "#print(type(tdata))\n",
    "#print(type(yu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata1=tdata1.withColumn(\"row_id\",monotonically_increasing_id())\n",
    "yu=yu.withColumn(\"row_id\",monotonically_increasing_id())\n",
    "r=tdata1.join(yu,on=[\"row_id\"],how='inner').drop(\"row_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "yc=r.select(\"movie_id\",\"features\",\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testdata=pd.read_csv('test.csv')\n",
    "test1=spark.createDataFrame(testdata)\n",
    "test1=rt.transform(test1)\n",
    "test1=remover.transform(test1)\n",
    "test1=htf.transform(test1)\n",
    "test1=idfmodel.transform(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n",
      "created labeled points\n",
      "training done\n",
      "testing done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "x=[]\n",
    "for z in range(0,len(m1)):\n",
    "    p = yc.rdd.map(lambda xu:LabeledPoint(xu.value[z],\n",
    "                                          Vectors.fromML(xu.features)))\n",
    "    print(\"created labeled points\")\n",
    "    lr = LogisticRegressionWithSGD.train(p,iterations=10)\n",
    "    print(\"training done\")\n",
    "    for zi in test1.collect():\n",
    "        x1=zi['movie_id']\n",
    "        y=lr.predict(Vectors.fromML(zi['features']))\n",
    "        pt=[str(x1),str(y)]\n",
    "        x.append(pt)\n",
    "    print(\"testing done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "presults={}\n",
    "count=0\n",
    "for jx in x:\n",
    "        jx[1]=str(int(float(jx[1])))\n",
    "        if jx[0] in presults.keys():\n",
    "            z=presults[jx[0]]\n",
    "            z.append(jx[1])\n",
    "        else:\n",
    "            z1=[]\n",
    "            z1.append(jx[1])\n",
    "            presults[jx[0]]=z1\n",
    "#print(presults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "for i in range(0,2):\n",
    "    a_file=open(\"results2.csv\",\"w\")\n",
    "    writer =csv.writer(a_file)\n",
    "    writer.writerow([\"movie_id\",\"predictions\"])\n",
    "    for k,v in presults.items():\n",
    "        x2=\"\"\n",
    "        for v1 in v:\n",
    "            x2=x2+\" \"+str(v1)\n",
    "            x2=x2.strip()\n",
    "        writer.writerow([k,x2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
